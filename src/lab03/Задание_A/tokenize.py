


tokonize_test_case = [
  "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä",        # ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"
  "hello,world!!!",    # ["hello", "world"]
  "–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ", # ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]
  "2025 –≥–æ–¥",          # ["2025", "–≥–æ–¥"]
  "emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"   # ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]
]



def tokenize(text: str) -> list[str]:
    import re
    
    p = r'\w+(?:-\w+)*'
    tokens = re.findall(p, text) # –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –Ω–∞—à–µ–π —Å—Ç—Ä–æ–∫–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Ö —Å–ø–∏—Å–æ–∫
    return tokens

for i in tokonize_test_case:
    print(tokenize(i))
    