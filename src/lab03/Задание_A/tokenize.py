tokonize_test_case = [
    "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä",  # ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"
    "hello,world!!!",  # ["hello", "world"]
    "–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ",  # ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]
    "2025 –≥–æ–¥",  # ["2025", "–≥–æ–¥"]
    "emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ",  # ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]
]


def tokenize(text: str) -> list[str]:
    import re

    p = r"\w+(?:-\w+)*"
    tokens = re.findall(
        p, text
    )  # –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –Ω–∞—à–µ–π —Å—Ç—Ä–æ–∫–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Ö —Å–ø–∏—Å–æ–∫
    return tokens


for i in tokonize_test_case:
    print(tokenize(i))
